import json
from uuid import UUID
from typing import AsyncGenerator, Optional
from google import genai

from .ai_providers import AbstractAIProvider, StreamedPart
from .ai_tools import available_tools
from service.db.base import Database
from service.db.models import ChatMessage # Import ChatMessage model
from service.utils.timing import debug_print
from .ai_helpers import pydantic_to_dict

class ChatOrchestrator:
    def __init__(self, user_id: UUID, session_id: UUID, db: Database, ai_provider: AbstractAIProvider, system_instructions: list[str]):
        self.user_id = user_id
        self.session_id = session_id
        self.db = db
        self.ai_provider = ai_provider
        self.system_instructions = system_instructions
        self.full_ai_response_text = ""
        self.available_store_ids: Optional[str] = None
        self.history: list = [] # Initialize history here

    async def _load_history(self):
        """Loads chat history from the database."""
        self.history = await self.db.chat.get_chat_messages(self.user_id, self.session_id)
        debug_print(f"[ChatOrchestrator] Loaded history from DB: {self.history}")

    async def stream_response(self, user_message_text: Optional[str]) -> AsyncGenerator[str, None]:
        """Orchestrates the chat flow, yielding SSE events."""
        
        # Load history at the start of the session if not already loaded
        if not self.history:
            await self._load_history()

        # If it's a new user message, append it to the in-memory history
        # The saving to DB is handled by the API endpoint (chat.py)
        if user_message_text and user_message_text.strip():
            self.history.append(ChatMessage(
                id=None, # ID will be generated by DB, not needed for in-memory history
                user_id=self.user_id,
                session_id=self.session_id,
                sender="user",
                message_text=user_message_text,
                timestamp=None, # Timestamp will be generated by DB
                tool_calls=None,
                tool_outputs=None,
                ai_response=None
            ))
            debug_print(f"[ChatOrchestrator] Appended new user message to in-memory history: {user_message_text}")


        ai_history = self.ai_provider.format_history(self.system_instructions, self.history, user_message_text)
        debug_print(f"[ChatOrchestrator] Formatted AI history before sending to provider: {ai_history}")

        while True:
            tool_call_info = None
            
            try:
                response_stream = self.ai_provider.generate_stream(ai_history)

                # Process stream for text or tool calls
                async for part in response_stream: # Iterate directly over StreamedPart objects
                    if part.type == "text":
                        self.full_ai_response_text += part.content
                    elif part.type == "tool_call":
                        tool_call_info = part.content
                    
                    yield part.to_sse()
                
                # If a tool was called, execute it
                if tool_call_info:
                    tool_output_info = await self._execute_tool(tool_call_info)
                    
                    # New: Extract store_ids if find_nearby_stores_for_user was called
                    if tool_output_info.get("name") == "find_nearby_stores_for_user" and tool_output_info.get("content") and "stores" in tool_output_info["content"]:
                        stores = tool_output_info["content"]["stores"]
                        self.available_store_ids = ",".join(map(str, [s["id"] for s in stores if "id" in s]))
                        debug_print(f"Extracted available_store_ids: {self.available_store_ids}")

                    # Add tool call and result to history for the next turn
                    # Use genai.types.Content with FunctionCall/FunctionResponse for history
                    ai_history.append(genai.types.Content(
                        role="model",
                        parts=[genai.types.FunctionCall(name=tool_call_info["name"], args=tool_call_info["args"])]
                    ))
                    # Ensure content is a plain dict or simple type, using json.dumps/loads for deep conversion
                    final_content = pydantic_to_dict(tool_output_info["content"])
                    try:
                        final_content = json.loads(json.dumps(final_content))
                    except (TypeError, json.JSONDecodeError) as e:
                        debug_print(f"WARNING: Failed deep conversion of tool_output_info content: {e}. Falling back to string.")
                        final_content = str(final_content) # Fallback if deep conversion fails

                    ai_history.append(genai.types.Content(
                        role="user",
                        parts=[genai.types.FunctionResponse(name=tool_output_info["name"], response=final_content)]
                    ))
                    
                    yield StreamedPart(type="tool_output", content=tool_output_info).to_sse()
                else:
                    # No tool call, conversation turn is over
                    break
            
            except Exception as e:
                debug_print(f"Exception in orchestrator stream: {e}")
                yield StreamedPart(type="error", content=str(e)).to_sse()
                break

        # Save final response and end the stream
        await self._save_final_response()
        yield StreamedPart(type="end", content={"session_id": str(self.session_id)}).to_sse()

    async def _execute_tool(self, tool_call_info: dict) -> dict:
        """Executes a tool and returns its output."""
        tool_name = tool_call_info["name"]
        tool_args = tool_call_info["args"]

        debug_print(f"Executing tool: {tool_name} with args: {tool_args}")
        
        # Inject user_id if needed by the tool
        if tool_name in ["find_nearby_stores_for_user", "get_user_personal_data"]:
             tool_args["user_id"] = self.user_id

        # New: If multi_search_tool is called and store_ids are available, inject them
        if tool_name == "multi_search_tool" and self.available_store_ids and "queries" in tool_args:
            debug_print(f"Injecting store_ids {self.available_store_ids} into multi_search_tool queries.")
            modified_queries = []
            for query_item in tool_args["queries"]:
                if query_item.get("name") == "search_products_v2" and "arguments" in query_item:
                    query_item["arguments"]["store_ids"] = self.available_store_ids
                modified_queries.append(query_item)
            tool_args["queries"] = modified_queries
            debug_print(f"Modified multi_search_tool args: {tool_args}")

        await self.db.chat.save_chat_message(
            user_id=self.user_id, session_id=self.session_id,
            message_text=f"Tool call: {tool_name}", is_user_message=False,
            tool_calls=tool_call_info
        )

        if tool_name not in available_tools:
            raise ValueError(f"Tool '{tool_name}' not found.")

        tool_function = available_tools[tool_name]
        output = await tool_function(**tool_args)
        
        tool_output_info = {"name": tool_name, "content": output}
        
        await self.db.chat.save_chat_message(
            user_id=self.user_id, session_id=self.session_id,
            message_text=f"Tool output for {tool_name}", is_user_message=False,
            tool_outputs=tool_output_info
        )
        return tool_output_info

    async def _save_final_response(self):
        if self.full_ai_response_text:
            debug_print(f"Saving final AI response: {self.full_ai_response_text}")
            await self.db.chat.save_chat_message(
                user_id=self.user_id, session_id=self.session_id,
                message_text=self.full_ai_response_text, is_user_message=False,
                ai_response=self.full_ai_response_text
            )
